# related articles

//机制：EAGLE使用强化学习和RNN模型确定算子设备放置时的最优算子分组。能够扩展已有强化学习的局限性，对比已有单个GPU、Tensorflow-Slim、层次规划和联合学习方法，能够取得较好效果。作为模型驱动的一种，能相交于启发式规则取得更好效果。
@inproceedings{lan2021eagle,
  title={EAGLE: Expedited Device Placement with Automatic Grouping for Large Models},
  author={Lan, Hao and Chen, Li and Li, Baochun},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={599--608},
  year={2021},
  organization={IEEE}
}


//机制：DUET首先使用通信感知的算子计算图划分方法减少计算子图之间的通信开销，然后使用性能检测工具监测这些子图的性能，最后使用贪心方法充分交叠CPU和GPU上的并行算子，最终克服当前放置方法不能充分利用异构资源并行加速比差异大的算子问题。实验与TVM、TensorFlow和Pytorch对比，从端到端宽模型并行执行延迟时间、调度方法的启发规则（贪心，装箱，随机，全局最优）、不同层的加速比和普通模型四个方面对比了本方法的有效性。
@inproceedings{zhang2021duet,
  title={DUET: A Compiler-Runtime Subgraph Scheduling Approach for Tensor Programs on a Coupled CPU-GPU Architecture},
  author={Zhang, Minjia and Hu, Zehua and Li, Mingqin},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={151--161},
  year={2021},
  organization={IEEE}
}

//模型：Mistify针对DL模型的设计、部署两个阶段差异较大，且部署在端云边环境效率较低的问题，设计了两层抽象，避免上层DL模型改变导致的频繁编译代码更新，使用基于树结构的整体算子运行环境适配以高效支持算子在云边环境下的放置，同时使用知识蒸馏方式提升设备间的通信效率，最后使用流式处理加速DL推理服务的响应时间，相交于传统部署方式能提升10倍的设计+部署时间。
@inproceedings{guo2021mistify,
  title={Mistify: Automating DNN Model Porting for On-Device Inference at the Edge.},
  author={Guo, Peizhen and Hu, Bo and Hu, Wenjun},
  booktitle={NSDI},
  pages={705--719},
  year={2021}
}

//策略：本文分析了同步、异步两类共7种算子状态同步方法在集群环境下的实际表现，通过模型精度、超参数敏感性、可伸缩性以及状态同步时的优化手段这四类来比较状态同步方法的预期效果，提出了同步方法伸缩性差、瓶颈资源在参数服务器的问题；提出了异步方法node间等待时间长的问题；提出了非等待反向传播方法在GPU上实际效果一般的问题。
@inproceedings{ko2021depth,
  title={An In-Depth Analysis of Distributed Training of Deep Neural Networks},
  author={Ko, Yunyong and Choi, Kibong and Seo, Jiwon and Kim, Sang-Wook},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={994--1003},
  year={2021},
  organization={IEEE}
}