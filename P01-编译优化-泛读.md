# 参考文献

//机制：EAGLE使用强化学习和RNN模型确定算子设备放置时的最优算子分组。能够扩展已有强化学习的局限性，对比已有单个GPU、Tensorflow-Slim、层次规划和联合学习方法，能够取得较好效果。作为模型驱动的一种，能相交于启发式规则取得更好效果。
@inproceedings{lan2021eagle,
  title={EAGLE: Expedited Device Placement with Automatic Grouping for Large Models},
  author={Lan, Hao and Chen, Li and Li, Baochun},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={599--608},
  year={2021},
  organization={IEEE}
}


//机制：DUET首先使用通信感知的算子计算图划分方法减少计算子图之间的通信开销，然后使用性能检测工具监测这些子图的性能，最后使用贪心方法充分交叠CPU和GPU上的并行算子，最终克服当前放置方法不能充分利用异构资源并行加速比差异大的算子问题。实验与TVM、TensorFlow和Pytorch对比，从端到端宽模型并行执行延迟时间、调度方法的启发规则（贪心，装箱，随机，全局最优）、不同层的加速比和普通模型四个方面对比了本方法的有效性。
@inproceedings{zhang2021duet,
  title={DUET: A Compiler-Runtime Subgraph Scheduling Approach for Tensor Programs on a Coupled CPU-GPU Architecture},
  author={Zhang, Minjia and Hu, Zehua and Li, Mingqin},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={151--161},
  year={2021},
  organization={IEEE}
}

//模型：Mistify针对DL模型的设计、部署两个阶段差异较大，且部署在端云边环境效率较低的问题，设计了两层抽象，避免上层DL模型改变导致的频繁编译代码更新，使用基于树结构的整体算子运行环境适配以高效支持算子在云边环境下的放置，同时使用知识蒸馏方式提升设备间的通信效率，最后使用流式处理加速DL推理服务的响应时间，相交于传统部署方式能提升10倍的设计+部署时间。
@inproceedings{guo2021mistify,
  title={Mistify: Automating DNN Model Porting for On-Device Inference at the Edge.},
  author={Guo, Peizhen and Hu, Bo and Hu, Wenjun},
  booktitle={NSDI},
  pages={705--719},
  year={2021}
}

//策略：本文分析了同步、异步两类共7种算子状态同步方法在集群环境下的实际表现，通过模型精度、超参数敏感性、可伸缩性以及状态同步时的优化手段这四类来比较状态同步方法的预期效果，提出了同步方法伸缩性差、瓶颈资源在参数服务器的问题；提出了异步方法node间等待时间长的问题；提出了非等待反向传播方法在GPU上实际效果一般的问题。
@inproceedings{ko2021depth,
  title={An In-Depth Analysis of Distributed Training of Deep Neural Networks},
  author={Ko, Yunyong and Choi, Kibong and Seo, Jiwon and Kim, Sang-Wook},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={994--1003},
  year={2021},
  organization={IEEE}
}

## 通过算子计算图替换的方式来间接实现算子的融合替换

@inproceedings{jia2019taso,
  title={TASO: optimizing deep learning computation with automatic generation of graph substitutions},
  author={Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={47--62},
  year={2019}
}

## 融合单个DL模型内部的相同算子

//模型：首次将延迟调度的思路引入算子粒度的调度，发现并行计算算子在不同执行时机对模型的数据流图有较大影响，并使用递归计算状态可变数据流子图最短完成时间达到整个大图的完成时间优化。该文献重点讨论了一个数据流内算子不同执行时机造成的影响，未来需要进一步考虑异构资源容量、通信开销、跨数据资源共享时对算子延迟调度造成的影响，或者一些新的共享效率提升潜力。此外，单一算子调度在50%的情况下并不能得到比TVM优化后更快的优化速度，未来可能需要两者结合，同时需要与rammer进行区分（overhead等方面考虑）。
@article{ding2021ios,
  title={Ios: Inter-operator scheduler for cnn acceleration},
  author={Ding, Yaoyao and Zhu, Ligeng and Jia, Zhihao and Pekhimenko, Gennady and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  year={2021}
}



## 多个相同模型自身重复运行时相同算子融合

//机制：集群环境中存在多种使用单卡加速的DL模型，这些模型存在相同算子（处理逻辑，输入输出形状），进行跨模型的算子融合能够提升资源利用率和GPU、TPU的吞吐量；HFTA还同时提出对应的训练时超参数更新方法，从而保证收敛性。其适用于多个相似DL模型使用单卡资源反复执行时资源利用率提升的问题。
@article{wang2021horizontally,
  title={Horizontally Fused Training Array: An Effective Hardware Utilization Squeezer for Training Novel Deep Learning Models},
  author={Wang, Shang and Yang, Peiming and Zheng, Yuxuan and Li, Xin and Pekhimenko, Gennady},
  journal={arXiv preprint arXiv:2102.02344},
  year={2021}
}

//对不同的层次中的算子进行融合，充分评估的多个DL模型的内存占用开销，使用动态规划方法优化资源使用
@inproceedings{cai2021optimus,
  title={Optimus: towards optimal layer-fusion on deep learning processors},
  author={Cai, Xuyi and Wang, Ying and Zhang, Lei},
  booktitle={Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
  pages={67--79},
  year={2021}
}

//循环融合算子
@inproceedings{niu2021dnnfusion,
  title={DNNFusion: accelerating deep neural networks execution with advanced operator fusion},
  author={Niu, Wei and Guan, Jiexiong and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
  booktitle={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages={883--898},
  year={2021}
}

## 对关键路径上的算子进行切分融合，细粒度算子切分（阿里巴巴）

//策略：DL模型基于数据并行方式，长期运行时会面临通信开销大导致的伸缩性问题，本文提出使用分布式SGD算法提升算子状态同步的效率，使用算子稀疏矩阵的并行切分运行缩短完成时间，能够对典型CNN网络进行分布式加速。此文论证了细粒度的算子并行状态同步能相交于粗粒度的数据并行，能减少通信开销从而保障更好的CNN训练伸缩性。
@inproceedings{demirci2021partitioning,
  title={Partitioning sparse deep neural networks for scalable training and inference},
  author={Demirci, Gunduz Vehbi and Ferhatosmanoglu, Hakan},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={254--265},
  year={2021}
}

//机制：FastT将MS-DF简化为有向无环图结构，通过静态分析感知算子执行时的关键路径，并对关键路径上的算子依次进行调度，最后切分关键路径上的部分算子，以进一步缩短训练完成时间。
@inproceedings{yi2020fast,
  title={Fast Training of Deep Learning Models over Multiple GPUs},
  author={Yi, Xiaodong and Luo, Ziyue and Meng, Chen and Wang, Mengdi and Long, Guoping and Wu, Chuan and Yang, Jun and Lin, Wei},
  booktitle={Proceedings of the 21st International Middleware Conference},
  pages={105--118},
  year={2020}
}

//机制：MS-DF的任务需要其所处作业内的调度才能执行，任务中每个算子的调度则往往通过第三方的深度学习库来进行，这2级调度器之间实际资源使用的差异往往会导致资源浪费，Rammer即据此设计了算子集合任务和异构可用资源单元这两类抽象，在编译阶段进行算子内和算子间的调度优化，避免调度的开销；同时对GPU、TPU和IPU等资源进行统一接口的抽象。可通过算子内并行计划和算子间约束关系最小化运行时间。
@inproceedings{ma2020rammer,
  title={Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks},
  author={Ma, Lingxiao and Xie, Zhiqiang and Yang, Zhi and Xue, Jilong and Miao, Youshan and Cui, Wei and Hu, Wenxiang and Yang, Fan and Zhang, Lintao and Zhou, Lidong},
  booktitle={14th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 20)},
  pages={881--897},
  year={2020}
}

## 算子自身资源调整

//机制：使用爬山算法预测算子的完成时间
@inproceedings{liu2019runtime,
  title={Runtime concurrency control and operation scheduling for high performance neural network training},
  author={Liu, Jiawen and Li, Dong and Kestor, Gokcen and Vetter, Jeffrey},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={188--199},
  year={2019},
  organization={IEEE}
}

## DL模型内不同算子的融合

//机制：现有算子融合方法仅是简单将不同算子运行逻辑端到端整合到一起，缺乏根据其自身运行逻辑的深度融合，本文将Relu作为mask过滤器与conv2D算子进行深度融合，预存权重与relu处理后结果，从而极大加速小模型的推理速度，同时层算子融合后也减少了算子数量。本方法适用于小模型同构资源的推理场景。
@inproceedings{olyaiy2021accelerating,
  title={Accelerating DNNs inference with predictive layer fusion},
  author={Olyaiy, MohammadHossein and Ng, Christopher and Lis, Mieszko},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={291--303},
  year={2021}
}

## 编译优化

//策略：收集不同算子的性能数据，构造数据库进行高效检索，同时面向新负载中的新算子，采用近似最优的schedule调整方法得到优化后的算子性能表现。
@inproceedings{yu2021lorien,
  title={Lorien: Efficient Deep Learning Workloads Delivery},
  author={Yu, Cody Hao and Shi, Xingjian and Shen, Haichen and Chen, Zhi and Li, Mu and Wang, Yida},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={18--32},
  year={2021}
}

//策略：提出高效算子编译优化的主要手段，考虑到不同编译优化阶段间可能造成的互相影响，并基于这种影响刻画优化费用函数，整合优化手段。
@inproceedings{phothilimthana2021flexible,
  title={A Flexible Approach to Autotuning Multi-Pass Machine Learning Compilers},
  author={Phothilimthana, Phitchaya Mangpo and Sabne, Amit and Sarda, Nikhil and Murthy, Karthik Srinivasa and Zhou, Yanqi and Angermueller, Christof and Burrows, Mike and Roy, Sudip and Mandke, Ketan and Farahani, Rezsa and others},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  pages={1--16},
  year={2021},
  organization={IEEE}
}