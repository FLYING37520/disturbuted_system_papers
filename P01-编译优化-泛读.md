# 参考文献

//机制：EAGLE使用强化学习和RNN模型确定算子设备放置时的最优算子分组。能够扩展已有强化学习的局限性，对比已有单个GPU、Tensorflow-Slim、层次规划和联合学习方法，能够取得较好效果。作为模型驱动的一种，能相交于启发式规则取得更好效果。
@inproceedings{lan2021eagle,
  title={EAGLE: Expedited Device Placement with Automatic Grouping for Large Models},
  author={Lan, Hao and Chen, Li and Li, Baochun},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={599--608},
  year={2021},
  organization={IEEE}
}


//机制：DUET首先使用通信感知的算子计算图划分方法减少计算子图之间的通信开销，然后使用性能检测工具监测这些子图的性能，最后使用贪心方法充分交叠CPU和GPU上的并行算子，最终克服当前放置方法不能充分利用异构资源并行加速比差异大的算子问题。实验与TVM、TensorFlow和Pytorch对比，从端到端宽模型并行执行延迟时间、调度方法的启发规则（贪心，装箱，随机，全局最优）、不同层的加速比和普通模型四个方面对比了本方法的有效性。
@inproceedings{zhang2021duet,
  title={DUET: A Compiler-Runtime Subgraph Scheduling Approach for Tensor Programs on a Coupled CPU-GPU Architecture},
  author={Zhang, Minjia and Hu, Zehua and Li, Mingqin},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={151--161},
  year={2021},
  organization={IEEE}
}

//模型：Mistify针对DL模型的设计、部署两个阶段差异较大，且部署在端云边环境效率较低的问题，设计了两层抽象，避免上层DL模型改变导致的频繁编译代码更新，使用基于树结构的整体算子运行环境适配以高效支持算子在云边环境下的放置，同时使用知识蒸馏方式提升设备间的通信效率，最后使用流式处理加速DL推理服务的响应时间，相交于传统部署方式能提升10倍的设计+部署时间。
@inproceedings{guo2021mistify,
  title={Mistify: Automating DNN Model Porting for On-Device Inference at the Edge.},
  author={Guo, Peizhen and Hu, Bo and Hu, Wenjun},
  booktitle={NSDI},
  pages={705--719},
  year={2021}
}

//策略：本文分析了同步、异步两类共7种算子状态同步方法在集群环境下的实际表现，通过模型精度、超参数敏感性、可伸缩性以及状态同步时的优化手段这四类来比较状态同步方法的预期效果，提出了同步方法伸缩性差、瓶颈资源在参数服务器的问题；提出了异步方法node间等待时间长的问题；提出了非等待反向传播方法在GPU上实际效果一般的问题。
@inproceedings{ko2021depth,
  title={An In-Depth Analysis of Distributed Training of Deep Neural Networks},
  author={Ko, Yunyong and Choi, Kibong and Seo, Jiwon and Kim, Sang-Wook},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={994--1003},
  year={2021},
  organization={IEEE}
}

## 通过算子计算图替换的方式来间接实现算子的融合替换

@inproceedings{jia2019taso,
  title={TASO: optimizing deep learning computation with automatic generation of graph substitutions},
  author={Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={47--62},
  year={2019}
}

## 融合单个DL模型内部的相同算子

//模型：首次将延迟调度的思路引入算子粒度的调度，发现并行计算算子在不同执行时机对模型的数据流图有较大影响，并使用递归计算状态可变数据流子图最短完成时间达到整个大图的完成时间优化。该文献重点讨论了一个数据流内算子不同执行时机造成的影响，未来需要进一步考虑异构资源容量、通信开销、跨数据资源共享时对算子延迟调度造成的影响，或者一些新的共享效率提升潜力。此外，单一算子调度在50%的情况下并不能得到比TVM优化后更快的优化速度，未来可能需要两者结合，同时需要与rammer进行区分（overhead等方面考虑）。
@article{ding2021ios,
  title={Ios: Inter-operator scheduler for cnn acceleration},
  author={Ding, Yaoyao and Zhu, Ligeng and Jia, Zhihao and Pekhimenko, Gennady and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  year={2021}
}



## 多个相同模型自身重复运行时相同算子融合

//机制：集群环境中存在多种使用单卡加速的DL模型，这些模型存在相同算子（处理逻辑，输入输出形状），进行跨模型的算子融合能够提升资源利用率和GPU、TPU的吞吐量；HFTA还同时提出对应的训练时超参数更新方法，从而保证收敛性。其适用于多个相似DL模型使用单卡资源反复执行时资源利用率提升的问题。
@article{wang2021horizontally,
  title={Horizontally Fused Training Array: An Effective Hardware Utilization Squeezer for Training Novel Deep Learning Models},
  author={Wang, Shang and Yang, Peiming and Zheng, Yuxuan and Li, Xin and Pekhimenko, Gennady},
  journal={arXiv preprint arXiv:2102.02344},
  year={2021}
}

//对不同的层次中的算子进行融合，充分评估的多个DL模型的内存占用开销，使用动态规划方法优化资源使用
@inproceedings{cai2021optimus,
  title={Optimus: towards optimal layer-fusion on deep learning processors},
  author={Cai, Xuyi and Wang, Ying and Zhang, Lei},
  booktitle={Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
  pages={67--79},
  year={2021}
}

//循环融合算子
@inproceedings{niu2021dnnfusion,
  title={DNNFusion: accelerating deep neural networks execution with advanced operator fusion},
  author={Niu, Wei and Guan, Jiexiong and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
  booktitle={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages={883--898},
  year={2021}
}

## 对关键路径上的算子进行切分融合，细粒度算子切分

//策略：DL模型基于数据并行方式，长期运行时会面临通信开销大导致的伸缩性问题，本文提出使用分布式SGD算法提升算子状态同步的效率，使用算子稀疏矩阵的并行切分运行缩短完成时间，能够对典型CNN网络进行分布式加速。此文论证了细粒度的算子并行状态同步能相交于粗粒度的数据并行，能减少通信开销从而保障更好的CNN训练伸缩性。
@inproceedings{demirci2021partitioning,
  title={Partitioning sparse deep neural networks for scalable training and inference},
  author={Demirci, Gunduz Vehbi and Ferhatosmanoglu, Hakan},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={254--265},
  year={2021}
}

//机制：FastT将MS-DF简化为有向无环图结构，通过静态分析感知算子执行时的关键路径，并对关键路径上的算子依次进行调度，最后切分关键路径上的部分算子，以进一步缩短训练完成时间。
@inproceedings{yi2020fast,
  title={Fast Training of Deep Learning Models over Multiple GPUs},
  author={Yi, Xiaodong and Luo, Ziyue and Meng, Chen and Wang, Mengdi and Long, Guoping and Wu, Chuan and Yang, Jun and Lin, Wei},
  booktitle={Proceedings of the 21st International Middleware Conference},
  pages={105--118},
  year={2020}
}

//机制：MS-DF的任务需要其所处作业内的调度才能执行，任务中每个算子的调度则往往通过第三方的深度学习库来进行，这2级调度器之间实际资源使用的差异往往会导致资源浪费，Rammer即据此设计了算子集合任务和异构可用资源单元这两类抽象，在编译阶段进行算子内和算子间的调度优化，避免调度的开销；同时对GPU、TPU和IPU等资源进行统一接口的抽象。可通过算子内并行计划和算子间约束关系最小化运行时间。
@inproceedings{ma2020rammer,
  title={Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks},
  author={Ma, Lingxiao and Xie, Zhiqiang and Yang, Zhi and Xue, Jilong and Miao, Youshan and Cui, Wei and Hu, Wenxiang and Yang, Fan and Zhang, Lintao and Zhou, Lidong},
  booktitle={14th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 20)},
  pages={881--897},
  year={2020}
}

//策略：在CPU的PS DL训练环境下，Harmony分析出单个训练作业分为拉取、计算和推送三个过程，面向多个训练作业，交叠这三个阶段，可以显著提升资源使用效率。其与Salus等工作的上GPU复用方法类似。未来可以在异构计算资源的细粒度精确共享方面进一步提升资源使用效率，从而可以缩短DL训练、推理时间。
@inproceedings{lee2021harmony,
  title={Harmony: A Scheduling Framework Optimized for Multiple Distributed Machine Learning Jobs},
  author={Lee, Woo-Yeon and Lee, Yunseong and Song, Won Wook and Yang, Youngseok and Kim, Joo Yeon and Chun, Byung-Gon},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
  pages={841--851},
  year={2021},
  organization={IEEE}
}

## 算子自身资源调整

//机制：使用爬山算法预测算子的完成时间
@inproceedings{liu2019runtime,
  title={Runtime concurrency control and operation scheduling for high performance neural network training},
  author={Liu, Jiawen and Li, Dong and Kestor, Gokcen and Vetter, Jeffrey},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={188--199},
  year={2019},
  organization={IEEE}
}

## DL模型内不同算子的融合

//机制：现有算子融合方法仅是简单将不同算子运行逻辑端到端整合到一起，缺乏根据其自身运行逻辑的深度融合，本文将Relu作为mask过滤器与conv2D算子进行深度融合，预存权重与relu处理后结果，从而极大加速小模型的推理速度，同时层算子融合后也减少了算子数量。本方法适用于小模型同构资源的推理场景。
@inproceedings{olyaiy2021accelerating,
  title={Accelerating DNNs inference with predictive layer fusion},
  author={Olyaiy, MohammadHossein and Ng, Christopher and Lis, Mieszko},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={291--303},
  year={2021}
}

## 编译优化

//策略：收集不同算子的性能数据，构造数据库进行高效检索，同时面向新负载中的新算子，采用近似最优的schedule调整方法得到优化后的算子性能表现。
@inproceedings{yu2021lorien,
  title={Lorien: Efficient Deep Learning Workloads Delivery},
  author={Yu, Cody Hao and Shi, Xingjian and Shen, Haichen and Chen, Zhi and Li, Mu and Wang, Yida},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={18--32},
  year={2021}
}

//策略：提出高效算子编译优化的主要手段，考虑到不同编译优化阶段间可能造成的互相影响，并基于这种影响刻画优化费用函数，整合优化手段。
@inproceedings{phothilimthana2021flexible,
  title={A Flexible Approach to Autotuning Multi-Pass Machine Learning Compilers},
  author={Phothilimthana, Phitchaya Mangpo and Sabne, Amit and Sarda, Nikhil and Murthy, Karthik Srinivasa and Zhou, Yanqi and Angermueller, Christof and Burrows, Mike and Roy, Sudip and Mandke, Ketan and Farahani, Rezsa and others},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  pages={1--16},
  year={2021},
  organization={IEEE}
}

//机制：分析不同算子在GPU和FPGA上的性能表现，发现GPU在不同输入下的功率波动比FPGA大，FGPA一般比GPU耗能低，以及有些算子在FPGA上跑的更快，基于此确定某些算子的调度；FPGA在运行时需要调整CU、SIMID、循环程度，组大小等FPGA参数，并根据这些参数构造性能模型；在实际运行时，重点最小化所有异构设备的性能开销和最大化训练性能（吞吐量）。基于这三个方面，实验验证了在多块CPU、FGPA组合情况下的表现。此种方法缺乏对GPU等编译层面的优化，可以与其他工作进行结合以达到更好的效果；同时，也可以提升GPU、FPGA的并行度优化。
@inproceedings{he2021enabling,
  title={Enabling energy-efficient DNN training on hybrid GPU-FPGA accelerators},
  author={He, Xin and Liu, Jiawen and Xie, Zhen and Chen, Hao and Chen, Guoyang and Zhang, Weifeng and Li, Dong},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={227--241},
  year={2021}
}

## CPU/GPU性能建模与分析优化

//机制：分析了NVIDIA GPU 采用most-room的线程调度策略，而不是round-robin，此种方法忽略SM的干扰，存在资源竞争现象。
@article{gilman2021demystifying,
  title={Demystifying the Placement Policies of the NVIDIA GPU Thread Block Scheduler for Concurrent Kernels},
  author={Gilman, Guin and Ogden, Samuel S and Guo, Tian and Walls, Robert J},
  journal={ACM SIGMETRICS Performance Evaluation Review},
  volume={48},
  number={3},
  pages={81--88},
  year={2021},
  publisher={ACM New York, NY, USA}
}

//机制：在DL负载下，通过优先级stream,MPS和分时复用三种GPU并行方式观察GPU被DL训练和推理服务共享时的性能表现，讨论影响共享性能和资源使用效率的关键因素，为后续DL负载的GPU调度提供基础
@article{gilman2021characterizing,
  title={Characterizing concurrency mechanisms for NVIDIA GPUs under deep learning workloads},
  author={Gilman, Guin and Walls, Robert J},
  journal={Performance Evaluation},
  volume={151},
  pages={102234},
  year={2021},
  publisher={Elsevier}
}

//Smaug基于端到端的思想充分评估DL作业的性能开销，基于gem5-Aladdin进行软硬件资源的模型，并通过实现ACP接口提升数据转移性能、使用多个加速器提升DL速度以及使用多线程加速数据处理过程三个措施提升DL的加速效果。此文表明DL会受到多种因素影响，需要充分评估应用多个阶段和不同层次的加速工作后才能提升DL训练、推理性能。
@article{xi2020smaug,
  title={Smaug: End-to-end full-stack simulation infrastructure for deep learning workloads},
  author={Xi, Sam and Yao, Yuan and Bhardwaj, Kshitij and Whatmough, Paul and Wei, Gu-Yeon and Brooks, David},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={17},
  number={4},
  pages={1--26},
  year={2020},
  publisher={ACM New York, NY, USA}
}

//此方法将DL模型切分不同的阶段，在各个阶段内通过动态优化方法调整其运行时间，以最大化吞吐量；然后在每个阶段内部，通过feature map算子的切分、对工作负载的分析等方式适应多种异构设备。此种方法只适用于CNN环境下多个负载的受限算子，且假设负载是泊松分布，存在一定的局限性。
@inproceedings{yang2021towards,
  title={Towards Efficient Inference: Adaptively Cooperate in Heterogeneous IoT Edge Cluster},
  author={Yang, Xiang and Qi, Qi and Wang, Jingyu and Guo, Song and Liao, Jianxin},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
  pages={12--23},
  year={2021},
  organization={IEEE}
}

//Tailored Profiling关注算子运行图的性能，从代码书写、编译、IR和机器代码多个层次上profile运行图性能，其核心是构造映射词典，实现代码、计算图、算子、IR和指令的多层关联，方便领域专家、开发人员和性能调优人员使用并改善性能；其在Umbra工具中进行了集成，且支持算子融合、常量折叠等等方面的性能建模，其profile开销和准确度也进行了验证。未来可以在异构资源和特定深度学习领域进行持续优化。
@inproceedings{beischl2021profiling,
  title={Profiling dataflow systems on multiple abstraction levels},
  author={Beischl, Alexander and Kersten, Timo and Bandle, Maximilian and Giceva, Jana and Neumann, Thomas},
  booktitle={Proceedings of the Sixteenth European Conference on Computer Systems},
  pages={474--489},
  year={2021}
}

## 框架工具

//TC通过深度学习编译的领域特定语言、多面底层IR表达式、schedule调整、贴片优化、线程块绑定、内存缓存优化、第三方NN库调用、配置自动调整和缓存等方面提供对DNN的全量支持能力，相较于TVM等其能力更加全面，但缺乏大规模的应用和开发。
@article{vasilache2019next,
  title={The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated GPU kernels, automatically},
  author={Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and Devito, Zachary and Moses, William S and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={16},
  number={4},
  pages={1--26},
  year={2019},
  publisher={ACM New York, NY, USA}
}

## 编译之上优化（优化DL的运行逻辑、包括prune、multi-exit、sparisity等）

//该方法在模型层面通过设置多个DL的结束运行出口避免无效的计算，然后在计算过程中的端云边设备使用多级队列控制计算作业，避免资源的无效占用或浪费。这两个方面的联合优化能实现1-17倍的加速。
@inproceedings{huang2021enabling,
  title={Enabling Low Latency Edge Intelligence based on Multi-exit DNNs in the Wild},
  author={Huang, Zhaowu and Dong, Fang and Shen, Dian and Zhang, Junxue and Wang, Huitian and Cai, Guangxing and He, Qiang},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
  pages={729--739},
  year={2021},
  organization={IEEE}
}

//该方法从待训练、推理的数据出发，通过分类复杂度和实例复杂度感知的两类方法，决定哪些数据在端边上进行直接计算，哪些需要传输到云端进行进一步运算。此方法只适用于有监督学习的DL场景，当数据不具备这些数据类型时，其方法难以有效适用。
@inproceedings{long2021complexity,
  title={Complexity-aware Adaptive Training and Inference for Edge-Cloud Distributed AI Systems},
  author={Long, Yinghan and Chakraborty, Indranil and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
  pages={573--583},
  year={2021},
  organization={IEEE}
}

//2PGraph依次使用数据并行和模型并行方法，实现对图数据的高效切分、采样和计算。其中，在切分和采样阶段，使用本地感知的规则降低采样的数据量，从而充分交叠计算和采样；在实际计算过程中，基于L临近方法进行高效缓存（缓存命中率达到100%），避免特征数据的频繁通信。实验在单个GPU、多个GPU、分布式训练、缓存优化效果和训练收敛速度这几个方面比较的工作的有效性。
@inproceedings{zhang20212pgraph,
  title={2PGraph: Accelerating GNN Training over Large Graphs on GPU Clusters},
  author={Zhang, Lizhi and Lai, Zhiquan and Li, Shengwei and Tang, Yu and Liu, Feng and Li, Dongsheng},
  booktitle={2021 IEEE International Conference on Cluster Computing (CLUSTER)},
  pages={103--113},
  year={2021},
  organization={IEEE}
}


## 综合使用多种手段进行算子优化编译

//机制：基于XLA，XTAT聚焦深度学习算子编译的多个阶段，基于性能模型评估各个编译阶段对性情造成的影响，选择最优的算子融合、算子贴片和重布局手段，提升5%-15%不等的推理性能。其在TPU上进行了实现，由于只考虑同构资源，在150个模型中，实际提升效果不高。结合异构资源性能，能够从多个维度优化算子性能仍然存在挑战。
@inproceedings{phothilimthana2021flexible,
  title={A Flexible Approach to Autotuning Multi-Pass Machine Learning Compilers},
  author={Phothilimthana, Phitchaya Mangpo and Sabne, Amit and Sarda, Nikhil and Murthy, Karthik Srinivasa and Zhou, Yanqi and Angermueller, Christof and Burrows, Mike and Roy, Sudip and Mandke, Ketan and Farahani, Rezsa and others},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  pages={1--16},
  year={2021},
  organization={IEEE}
}

//策略：DaCeML提供相对完备的深度学习模型优化可视化工具，组织为若干特定阶段，提供算子融合等方面的优化能力。
@article{rausch2021data,
  title={A Data-Centric Optimization Framework for Machine Learning},
  author={Rausch, Oliver and Ben-Nun, Tal and Dryden, Nikoli and Ivanov, Andrei and Li, Shigang and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2110.10802},
  year={2021}
}