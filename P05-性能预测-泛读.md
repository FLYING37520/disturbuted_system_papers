# 参考文献

//need to review.
@inproceedings{romero2021infaas,
  title={INFaaS: Automated Model-less Inference Serving},
  author={Romero, Francisco and Li, Qian and Yadwadkar, Neeraja J and Kozyrakis, Christos},
  booktitle={2021 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 21)},
  pages={397--411},
  year={2021}
}

//策略：基于DL模型训练过程中的资源交叠使用特点，细粒度分析DL算子运行时的交叠时机，预测最优的交叠位置，避免内存资源的过早分配，进一步提升GPU显存的使用效率，提供内存感知的安全资源分配。
@inproceedings{lim2021zico,
  title={Zico: Efficient $\{$GPU$\}$ Memory Sharing for Concurrent $\{$DNN$\}$ Training},
  author={Lim, Gangmuk and Ahn, Jeongseob and Xiao, Wencong and Kwon, Youngjin and Jeon, Myeongjae},
  booktitle={2021 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 21)},
  pages={161--175},
  year={2021}
}

//策略：使用白盒数据增量机制加速DL模型的训练过程。
@inproceedings{lee2021refurbish,
  title={Refurbish Your Training Data: Reusing Partially Augmented Samples for Faster Deep Neural Network Training},
  author={Lee, Gyewon and Lee, Irene and Ha, Hyeonmin and Lee, Kyunggeun and Hyun, Hwarim and Shin, Ahnjae and Chun, Byung-Gon},
  booktitle={2021 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 21)},
  pages={537--550},
  year={2021}
}

//机制+策略：根据DL训练时的周期迭代特性预测周期训练的时间；根据算子的伸缩特性预测DL模型在6个不同GPU型号上的运行时间。
@inproceedings{geoffrey2021habitat,
  title={Habitat: A Runtime-Based Computational Performance Predictor for Deep Neural Network Training},
  author={Geoffrey, X Yu and Gao, Yubo and Golikov, Pavel and Pekhimenko, Gennady},
  booktitle={2021 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 21)},
  pages={503--521},
  year={2021}
}

//机制：Mars使用图神经网络加速算子的设备放置过程，同时保障放置后的推理延迟。其首次将预训练的图编码器应用于强化学习框架，用于刻画算子的依赖结构；使用算子计算子图层次的序列放置器去学习最优的设备放置。对于达摩行能获得到接近27%的性能提升，对于小模型能快速得到最优放置结果。这是比较偏算法的工作，缺乏算子实际表现的启发式规则。
@article{lan2021accelerated,
  title={Accelerated Device Placement Optimization with Contrastive Learning},
  author={Lan, Hao and Chen, Li and Li, Baochun},
  year={2021} 
}

//Ray的分布式future机制为细粒度任务（function）的高效调度提供了支撑，能相交于单一RPC、分布式内存PRC、RPC+future三种方式提升更多的函数并行能力，同时同步开销。此外Ray的Ownership组件也提供了故障恢复等能力，未来可以进一步对TVM等算子进行分布式异构编译优化。
@inproceedings{wang2021ownership,
  title={Ownership: A Distributed Futures System for Fine-Grained Tasks.},
  author={Wang, Stephanie and Liang, Eric and Oakes, Edward and Hindman, Benjamin and Luan, Frank Sifei and Cheng, Audrey and Stoica, Ion},
  booktitle={NSDI},
  pages={671--686},
  year={2021}
}

//机制：Gillis使用无服务器计算模式来加速模型推理服务，由于无服务计算模型的受限资源使用和频繁的通信模式，存在服务器计算函数调度复杂、通信开销大、延迟和函数部署成本难以控制的问题。据此，Gillis通过fork join模式控制算子所在函数的调度顺序，使用动态规划方法评估不同算子划分组情况下的推理延迟，同时建模DL推理的运行成本，最后强化学习方法平衡成本和延迟。实验证明其具有一定的伸缩性，但其只在同构资源上验证了有效性。
@inproceedings{yu2021gillis,
  title={Gillis: Serving Large Neural Networks in Serverless Functions with Automatic Model Partitioning},
  author={Yu, Minchen and Jiang, Zhifeng and Ng, Hok Chun and Wang, Wei and Chen, Ruichuan and Li, Bo},
  booktitle={41st IEEE International Conference on Distributed Computing Systems},
  year={2021}
}

//机制：D^3首先根据模型局部算子的运行时间和层间算子的通信开销进行水平切分（划分为三个部分），然后针对边缘段的层内算子进行垂直切分，在满足推理延迟的同时，提升边缘端设备的资源使用效率，平均能缩短3.4倍的推理延迟和3.6倍的通信开销。
@article{zhang2021dynamic,
  title={Dynamic DNN Decomposition for Lossless Synergistic Inference},
  author={Zhang, Beibei and Xiang, Tian and Zhang, Hongxuan and Li, Te and Zhu, Shiqiang and Gu, Jianjun},
  journal={arXiv preprint arXiv:2101.05952},
  year={2021}
}

//机制：NPBench面向生物化学领域，定义了新的算子评估框架，其主要能够支持多种不同的编译平台，支持主流NP操作，通过注解的形式提供多重编译选项，其覆盖到了conv2D，lu等40多个主流算子。但不支持异构资源。
@inproceedings{ziogas2021npbench,
  title={NPBench: a benchmarking suite for high-performance NumPy},
  author={Ziogas, Alexandros Nikolaos and Ben-Nun, Tal and Schneider, Timo and Hoefler, Torsten},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={63--74},
  year={2021}
}

//策略：主要分析了NLP DL模型中算子的特点，包括算子依赖的多分支、计算和通信不能有效交叠这三方面， lbann使用数据与算子子图混合并行的方式提升transform模型的伸缩性。
@inproceedings{jain2021super,
  title={SUPER: SUb-Graph Parallelism for TransformERs},
  author={Jain, Arpan and Moon, Tim and Benson, Tom and Subramoni, Hari and Jacobs, Sam Ad{\'e} and Panda, Dhabaleswar K and Van Essen, Brian},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={629--638},
  year={2021},
  organization={IEEE}
}

//机制：Mopt从卷积算子计算逻辑的循环优化出发，指出张量不同循环层次的“贴片”使用不同内存（缓存）的情况是影响其自身执行时间的关键，其通过L1，L2，L3和内存之间数据移动的分析，建模数据传输的性能开销，简化最优贴片和变形最优配置的决策复杂度，能刻画传统黑盒方法受限搜索空间的局限性，相较于TVM能最多提升2倍的加速比。Mopt适用场景较为有限，目前只支持CPU和卷积算子，可以与TVM结合进一步提升其适用范围。
@inproceedings{li2021analytical,
  title={Analytical characterization and design space exploration for optimization of CNNs},
  author={Li, Rui and Xu, Yufan and Sukumaran-Rajam, Aravind and Rountev, Atanas and Sadayappan, P},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={928--942},
  year={2021}
}

## 多个相同模型自身错峰运行，并交叠资源

//策略：在cuda驱动层面上进行算子并行运行（空间复用）的研究，不关心算子层次优化的基础上，拦截算子对CUDA的调用，分析调用之间的依赖关系，从调用粒度的运行时解决单块GPU的资源复用问题，通过计算时间和通信时间的交叠比例，通信时间和计算时间的交叠比例来计算实际复用提升效果。从自定义CUDA驱动角度也能在一定程度上提升算子的执行效率，但仍然需要在多GPU环境，已经tensorflow、tvm等环境下适用性。
@inproceedings{parravicini2021dag,
  title={DAG-based Scheduling with Resource Sharing for Multi-task Applications in a Polyglot GPU Runtime},
  author={Parravicini, Alberto and Delamare, Arnaud and Arnaboldi, Marco and Santambrogio, Marco D},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={111--120},
  year={2021},
  organization={IEEE}
}

//模型：当前的GPU共享主要针对多DL模型，wavelet实现对同一模型在数据、模型并行时的资源共享，控制一个DL模型的训练执行时机来交替错峰使用GPU资源，同时实现内存（容量错峰）和计算资源（基于cuda stream实现计算错峰）的分时复用，提升资源使用率和运行完成时间。这种调度算法，假设算子划分已知和确定，不对算子进行调整优化，是tvm等工作的补充。
@article{wang2021wavelet,
  title={Wavelet: Efficient DNN Training with Tick-Tock Scheduling},
  author={Wang, Guanhua and Wang, Kehan and Jiang, Kenan and Li, Xiangjun and Stoica, Ion},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  year={2021}
}

## 不同模型共享内存资源，交叠资源

//机制：分析算子的内存资源使用特点，通过内存容量的复用，不同算子并行执行时可能会存在一定程度的交叠
@article{yu2019salus,
  title={Salus: Fine-grained gpu sharing primitives for deep learning applications},
  author={Yu, Peifeng and Chowdhury, Mosharaf},
  journal={arXiv preprint arXiv:1902.04610},
  year={2019}
}

## 分析单个模型的算子并行能力，进行异构资源交叠使用

@article{wang2020exploiting,
  title={Exploiting parallelism opportunities with deep learning frameworks},
  author={Wang, Yu Emma and Wu, Carole-Jean and Wang, Xiaodong and Hazelwood, Kim and Brooks, David},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={18},
  number={1},
  pages={1--23},
  year={2020},
  publisher={ACM New York, NY, USA}
}

## 综合使用多种手段进行算子优化编译

//机制：基于XLA，XTAT聚焦深度学习算子编译的多个阶段，基于性能模型评估各个编译阶段对性情造成的影响，选择最优的算子融合、算子贴片和重布局手段，提升5%-15%不等的推理性能。其在TPU上进行了实现，由于只考虑同构资源，在150个模型中，实际提升效果不高。结合异构资源性能，能够从多个维度优化算子性能仍然存在挑战。
@inproceedings{phothilimthana2021flexible,
  title={A Flexible Approach to Autotuning Multi-Pass Machine Learning Compilers},
  author={Phothilimthana, Phitchaya Mangpo and Sabne, Amit and Sarda, Nikhil and Murthy, Karthik Srinivasa and Zhou, Yanqi and Angermueller, Christof and Burrows, Mike and Roy, Sudip and Mandke, Ketan and Farahani, Rezsa and others},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  pages={1--16},
  year={2021},
  organization={IEEE}
}

//策略：DaCeML提供相对完备的深度学习模型优化可视化工具，组织为若干特定阶段，提供算子融合等方面的优化能力。
@article{rausch2021data,
  title={A Data-Centric Optimization Framework for Machine Learning},
  author={Rausch, Oliver and Ben-Nun, Tal and Dryden, Nikoli and Ivanov, Andrei and Li, Shigang and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2110.10802},
  year={2021}
}