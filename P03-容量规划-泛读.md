# 参考文献

//基于贝叶斯优化方法，以深度学习训练作业的数据并行形式为基础，预测需要的AWS实例类型和实例数量
@inproceedings{yi2020not,
  title={Not All Explorations Are Equal: Harnessing Heterogeneous Profiling Cost for Efficient MLaaS Training},
  author={Yi, Jun and Zhang, Chengliang and Wang, Wei and Li, Cheng and Yan, Feng},
  booktitle={2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={419--428},
  year={2020},
  organization={IEEE}
}

// 使用对比学习算法，预测深度学习训练作业在模型并行模式下的资源使用，并给出最优的设备使用结果（同构资源）
@inproceedings{lan2021accelerated,
  title={Accelerated Device Placement Optimization with Contrastive Learning},
  author={Lan, Hao and Chen, Li and Li, Baochun},
  booktitle={50th International Conference on Parallel Processing},
  pages={1--10},
  year={2021}
}

// Pesto使用算子粒度的建模，估算算子的运行时间，并考虑在模型并行时的算子放置与调度结果
@inproceedings{hafeez2021towards,
  title={Towards optimal placement and scheduling of DNN operations with Pesto},
  author={Hafeez, Ubaid Ullah and Sun, Xiao and Gandhi, Anshul and Liu, Zhenhua},
  booktitle={Proceedings of the 22nd International Middleware Conference},
  pages={39--51},
  year={2021}
}

// 在云服务之上，如果通过算子预测整个模型的一次运行时间
@inproceedings{hafeez2020empirical,
  title={Empirical Analysis and Modeling of Compute Times of CNN Operations on AWS Cloud},
  author={Hafeez, Ubaid Ullah and Gandhi, Anshul},
  booktitle={2020 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={181--192},
  year={2020},
  organization={IEEE}
}

//机制：nn-Meter将深度学习模型及其算子进行细粒度的划分，能够自动将算子划分为需要的运算单元(由多个算子融合而来)，同时通过数据采样（包含算子的输入数据量和参数维度）便于精准估算算子的运行时间，能够达到接近90%的准确度。NN-meter的局限性在于其只能支持离线融合计算，同时一般算子独占异构资源（边缘环境下有其合理性） ；在可用资源动态变化和算子并发执行下（共享异构资源）的场景还需要扩展，其算子可以预测的对象也仅限于同构资源上的CNN算子，同时，数据采样所需要的时间也较长（1.4-2.5天）。
@inproceedings{zhang2021nn,
  title={nn-Meter: towards accurate latency prediction of deep-learning model inference on diverse edge devices},
  author={Zhang, Li Lyna and Han, Shihao and Wei, Jianyu and Zheng, Ningxin and Cao, Ting and Yang, Yuqing and Liu, Yunxin},
  booktitle={Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
  pages={81--93},
  year={2021}
}

//机制：采用元学习模型评估DL推理在不同CPU、GPU云服务VM机型上的性能表现，重点调整CPU核数、GPU型号、GPU显存和Batch size，降低推理服务的部署开销。
@inproceedings{wang2021morphling,
  title={Morphling: Fast, Near-Optimal Auto-Configuration for Cloud-Native Model Serving},
  author={Wang, Luping and Yang, Lingyun and Yu, Yinghao and Wang, Wei and Li, Bo and Sun, Xianchao and He, Jian and Zhang, Liping},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={639--653},
  year={2021}
}

//DL负载使用虚拟机时，特别是面对流媒体负载，会经常遇到CPU、GPU多个VM依次使用的情况。Scrooge对此使用成本感知的虚拟机选择和时空复用方法，基于预测的运行时间剥离资源评估、分配和放置这些阶段，能够降低DL推理成本，其实验在三种云上验证了有效性。
@inproceedings{hu2021scrooge,
  title={Scrooge: A Cost-Effective Deep Learning Inference System},
  author={Hu, Yitao and Ghosh, Rajrup and Govindan, Ramesh},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={624--638},
  year={2021}
}

//面向深度学习推荐模型的负载的大内存使用需要，在单通道环境使用密集矩阵、稀疏矩阵分别处理的方案进行性能分析；在多通道环境使用通信均衡的方式提升通信的并行能力，在训练时也使用基于float 16的混合精度参数同步方法，能提升最多110X的训练速度。
@inproceedings{kalamkar2020optimizing,
  title={Optimizing deep learning recommender systems training on CPU cluster architectures},
  author={Kalamkar, Dhiraj and Georganas, Evangelos and Srinivasan, Sudarshan and Chen, Jianping and Shiryaev, Mikhail and Heinecke, Alexander},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2020},
  organization={IEEE}
}

//RIBBON作为黑盒配置优化推荐方法，使用贝叶斯优化关注成本和服务质量满足程度，采用同构、异构配置aws实例型号相结合的搜索方法优化DL inference的资源选择。
@inproceedings{li2021ribbon,
  title={RIBBON: cost-effective and qos-aware deep learning model inference using a diverse pool of cloud computing instances},
  author={Li, Baolin and Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay and Gettings, Karen and Tiwari, Devesh},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--13},
  year={2021}
}

# 存在问题

目前DL训练作业的配置推荐，存在以下局限性：
1. 只能针对数据并行场景建模资源使用情况，对模型并行几乎没有支撑；
2. 完成时间的预测一般针对DL训练作业只能使用同构资源。
3. 模型并行的设备放置过程，也只能使用同构资源。

# 设计方案

面向一个DL训练作业的模型并行场景，同时推荐不同的资源型号和数量：

1. 构造DL训练作业的划分方法，不同划分子作业中存在不同的算子，其具有不同的运行时间，和在异构资源上不同的加速情况；
2. 构造自优化搜索算法，对每部分算子分别建模，寻找其最优的GPU型号，以及需要的整体数量；
3. 实验对比与一般只推荐同构资源的工作，有完成时间和推荐成本方面的提升。
