# Title

<!-- 此部分是论文标题-->
DL2 : A Deep Learning-Driven Scheduler for Deep Learning Clusters

## Website
<!-- 网址，有DOI的建议用DOI地址-->
https://ieeexplore.ieee.org/abstract/document/9328612
## Citing

<!-- 引用格式，建议使用latex格式-->
@article{peng2021dl2,
  title={Dl2: A deep learning-driven scheduler for deep learning clusters},
  author={Peng, Yanghua and Bao, Yixin and Chen, Yangrui and Wu, Chuan and Meng, Chen and Lin, Wei},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={32},
  number={8},
  pages={1947--1960},
  year={2021},
  publisher={IEEE}
}

## Brief Introduction

<!-- 通过三五句话描述这篇文章，包括 1. 论文的应用场景；2. 论文克服已有方法的局限性；3. 论文主要的技术手段； 4. 论文的预期结果 -->
深度学习训练任务调度场景。目前的调度方法大多数都依靠用户指定具体的资源数量，并且资源的分配是静态的。而现有的启发式方法大多数都对任务的训练速度与它需要的资源关系做了建模，这种建模耦合度高，并且精度不足（Optimus）。本文主要采取了offline的监督学习与online的强化学习结合的方式。offline的监督学习提供一个online强化学习的基础模型，随后在online训练过程中，不断通过feedback优化模型，达到更优的资源调度的目的。

## Key Methodology

本文主要讨论分布式PS训练架构。
<!-- 分点写，论述论文中主要技术手段的实施过程 -->
1. 动机

    现有工作流：
    
    用户提交一个任务，指定它需要多少PS和workers，以及指定每个worker/PS需要的资源数量。

    - 用户选定合适的worker/PS数量存在困难。

        数据表明，不同任务在不同的worker/PS数量下具有不同的加速比。

    - 资源静态分配

        在先前的调度器，当job释放GPU后，释放的GPU不会被其他job利用起来。

        所以本文希望能够动态调整worker/PS的数量，最大化利用率。
        
        并且不需要用户指定worker/PS数量。而是基于全局的资源可用情况以及该job的性能，来决定最优的worker/PS数量。

    - 现有的专家知识的启发式算法具有缺点

        现有的白盒方法，对于训练速度到job需要的资源关系做了建模。

        这种方法的问题：
        1. 为了达到精确性，这个建模需要与ML框架的实现或workload的模式紧密耦合。
            
            重新训练模型耗时很长并且每当框架变化时，都需要重新建模。

        2. 前面的这些性能模型通常没有考虑到多租户GPU集群的job之间的干扰。如SLAQ和Optimus假设没有网络竞争。
            
            分析trace表明：同一个job在多次运行中的completion时间差距很大，这就是收到了其他任务干扰的结果。

2. 现有DRL（深度强化学习）在资源分配上的使用的问题

    现有的DRL-based调度器通常使用线下的调度器生成trace。它们依赖一个特定的 资源-性能 模型，来用它衡量job在某个资源分配下的进度。随后用这些数据训练强化学习的模型。在将来的调度中只用它来inference。

    但是这种资源-性能模型的不精确性会导致训练出的模型精度差。

    虽然可以用真实的trace数据做训练，但是由于资源分配的空间很大，真实trace通常不能涵盖每个job在每种资源分配下的性能。

    所以本文希望通过online的强化学习方式，在线上不断地获得真实的资源分配情况的反馈，从而获得更精确的模型。为了避免在一开始的时候，模型精度不够的问题，首先在offline训练一个policy NN模型，使得它能够尽可能拟合一个普通调度器的行为模式。在线上直接使用它作为强化学习的policy模型的初始模型。这样不至于在初始时它的精度太差。

3. 调度器总览

    1. offline监督学习

        线下的监督学习的意义：
        
        使用一个现有的调度器做决策，将DRL里面的policy模型训练到使得它能够做出与现有调度器的类似决策，避免它一开始放到线上的性能很差的情况。

    2. online 强化学习

        线上的强化学习使用时间槽fashion。
        
        每个时间槽是一个调度的间隔。（1小时）
        
        在调度开始时，policy神经网络使用现有的并行jobs作为input state，产出一个每个job worker数量和PS数量。
        
        Job的训练进度在每个时间槽的结尾被观测，并作为reward来提升现有的policy神经网络的精度。

    3. 动态扩缩容。（调整worker和PS数量）

        因为每次调度结果是包括了每个job的worker/PS数量，需要能够动态更改它们的数量，所以本文修改了MXNet框架，增加了动态扩缩容的功能。

4. 设计

    1. policy网络

        强化学习中的状态如何表示？

         1. x矩阵：用一个J x L的矩阵，表示job的类型特征。
            
            J是并行jobs数量的最大值。L表示job类型的最大值。
            
            考虑将一个job的DNN结构类似的座位同一种job type。
            
            比如：基于预训练模型的微调时很常见的，它们被视为同一种类型。每一行的一个向量xi表示一个类型的编码。

         2. d向量：
            
            长度为J的向量，编码了每个job在集群中已经运行过的time slot的数量。

         3. e向量：
            
            长度为J的向量，编码了每个job的剩余epochs。

         4. r向量：
            
            长度为J的向量，表示每个job在当前时间片中，消耗资源最多的类型的在同类型资源中的占比（资源支配率）。
            
            （例如：任务i占用3个GPU，2个CPU，而集群一共有4个GPU，10个CPU，则GPU的支配率为3/4，CPU的为2/10，所以取GPU的支配率）。

         5. w和u向量：
            
            长度为J的向量。分别为每个job的worker/PS的数量（在当前时间槽中）。

    2. Action定义

        policy NN将产出一个action的概率。
        
        最直接的设计就是，允许让每个action去指定全部并行job的worker/PS数量，但是很明显，这样设计回导致action的数量指数级增加。很大的action空间会让训练难以收敛。

        所以需要简化action定义。
        
        本文将action数量固定到了3 x J + 1。每个action可能是如下几种情况：
         1. (I, 0)为任务I增加1个worker。
         2. (I, 1)为任务I增加1个PS。
         3. (I, 2)为任务I增加1个worker和PS。
         4. 空过。

        但是很明显这样做是不完整的，每一个inference结果只有一个增量。所以，本文运行多轮的inference，以便于在每个time slot产出一个完整的资源分配决定。即，在每次产出一个action后，更新状态s，然后使用NN来产出下一个action，迭代直到资源用光，或者void action被产出。

    3. policy NN结构

        简单的几层全连接，加上ReLU。输出使用Softmax。

    4. offline监督学习

        使用SGD来更新policy NN，损失函数为，某个现存的调度器的调度决定与policy NN的调度决定的交叉熵。

    5. online的强化学习

        如何定义Reward函数？

        由于目标是最小化JCT，所以使用JCT作为Reward是天然的想法，但是JCT需要当任务完成时才能获得，而且很多任务的完成需要很久，所以使用JCT作为reward会让feedback的delay很长，所以他们很难在早期的决定上给出有效的指导。

        所以设计了per-time-slot的reward函数。可以让RL模型频繁的更新，能够更快收敛。

        该reward函数为：将每个job在当前time-slot的执行epochs，进行归一化后（除以总epochs），再求和。

        背后逻辑就是：运行的epochs越多，距离他完成就越近。所以能够增量式的达到最小化JCT的目的。

        需要做归一化，是需要避免受到那些较长的job的影响。（即，需要看这个job自己的完成比例，而不是看绝对的epochs数量。）

        随后使用标准的DRL流程进行梯度下降训练。

        - 针对DRL能够更加精确和快速收敛的优化
        
          - 使用Actor-Critic方法，使得每次迭代的梯度的方差更小。
          
          - Job-Aware Exploration：

            需要保证action的空间被explored得足够宽。否则容易陷入局部最优。
            
            所以添加了一个正则项。用这种方法能够增大网络输出的熵。

            在训练中，发现了大量比较差的exploration（比如分配worker但是不分配PS）。为了提升效率，使用epsilon-greedy方法。每当使用policy NN进行inference时，检查input state，如果是一个poor的input state：有1-epsilon的概率，使用它的inference结果，而剩余概率使用提前指定的针对这种poor的state的一个action。

          - Experience Replay

            经验重放机制：
            
            已经被证明的现象是：样本相互之间的关系会阻止actor-critic模型和好的policy的收敛。
            
            在本文的RL中，如果policy网络发现给某些job分配更多worker会提升reward，那么在后续的序列中会一致尝试给它增加worker，这会减少拓展其他训练job的空间。

            所以本文的方法：通过经验重放：维护一个replay buffer。在一个较长的时间窗口内记录样本，当time slot结束时，并不是选择该time slot的全部样本，而是从replay buffer中选择一个mini-batch的样本，这个样本可能来自多个之前的time-slot。这样能够避免陷入只关注少数job的情况。

5. 弹性扩缩容

    基于MXNet，增加了能够随时添加PS/worker的动态扩缩容机制。

    在MXNet中，添加一个coordinator组件，能够负责协调维护PS/worker之间的状态，并与调度器之间进行交互。

    为了解决在并发过程中的冲突，使用了乐观的并发冲突解决方案。
    为参数维护一个版本号。

    使用scaling clock机制帮助完成PS节点加入，即，当每个job的版本号达到scaling clock的值时，暂停pull/push参数，等待coordinator的协调。在达到scaling clock之前，有任意多的新的PS加入，都可以在这期间做好初始化工作。当scaling clock到达后，将参数在所有PS上进行重新的最优分配。

        

## Data sets & Experimental Design

<!-- 撰写实验环境的设置，实验的对象，实验的比较方面，以及实验的结果（不要列举数据，要概括谈） -->
- 实验环境

    13个GPU/CPU服务器，同构。以及模拟器。

    使用8个在MXNet官方教学中的模型。

- 对比对象

    - Dominant Resource Fairness(DRF)

        要比较它是因为，在本文中，基准的监督模型是使用这个调度器来训练的。

    - Tetris
    - Optimus
    - OfflineRL

- 对比方面

    - 性能

        - 与其他方法对比
            
            JCT

        - 与自己对比

            分别比较：
            - 只使用offline训练的监督模型
            - 只使用online的RL（不使用offline的监督模型作为基础）
            - 与DRF比较（因为监督模型是基于它的）
            - 与完全体比较

        - 分析Seq2Seq任务在执行过程中，它的workers和PS数量变化。
        - 分析弹性扩缩容的overhead


    - 泛化能力
      - 将同一任务的完成时间进行缩放（同一任务的时间变化，这对于白盒方法是不利的），模拟它在真实场景中的变化。

        与Optimus比较，发现Optimus对任务完成时间的变化更敏感

      - 对训练任务的epoch预估精度变化上的性能比较。
      - 对未知任务类型的泛化能力。
      - 对监督学习的模型使用其他调度策略的比较。


                    

## Conclusion And Future Work

<!-- 作者或者阅读者对本文工作的总结，以及未来可能的改进方向 -->
本文将深度强化学习利用到深度学习训练任务的集群资源调度分配中来。采取了线下监督学习+线上强化学习的方式，并实现了MXNet框架上的训练任务弹性伸缩。能够有效减小jct，并提升集群利用效率。