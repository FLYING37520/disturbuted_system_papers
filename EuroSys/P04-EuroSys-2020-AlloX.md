# Title

<!-- 此部分是论文标题-->
AlloX: compute allocation in hybrid clusters

## Website
<!-- 网址，有DOI的建议用DOI地址-->
https://dl.acm.org/doi/10.1145/3342195.3387547

## Citing

<!-- 引用格式，建议使用latex格式-->
@inproceedings{le2020allox,
  title={AlloX: compute allocation in hybrid clusters},
  author={Le, Tan N and Sun, Xiao and Chowdhury, Mosharaf and Liu, Zhenhua},
  booktitle={Proceedings of the Fifteenth European Conference on Computer Systems},
  pages={1--16},
  year={2020}
}

## Brief Introduction

<!-- 通过三五句话描述这篇文章，包括 1. 论文的应用场景；2. 论文克服已有方法的局限性；3. 论文主要的技术手段； 4. 论文的预期结果 -->
本文设计实现了一个在异构场景，多任务配置情况下的深度学习模型训练的任务调度器。本文针对的调度都只占用单个计算资源（如CPU，GPU）。每个任务在不同计算资源上具有不同的配置以及性能。它的主要调度目标为1. 减少平均JCT 2. 用户公平性。本文将一批任务同时到达的这种简单情况使用双向图匹配的方式建模，能够在多项式时间内得到最优解，并将它扩展到了任务随时到来的情况。针对用户公平性也对算法进行了适配。最终能够有效减小JCT并尽可能地提供了公平性。

## Key Methodology

<!-- 分点写，论述论文中主要技术手段的实施过程 -->
本文在描述异构场景时只针对了CPU和GPU。但是可以算法扩展到异构GPU和异构CPU中。

本文仅针对每个训练任务独占一个计算资源的情况，分布式训练和space sharing均没有考虑。

本文提到的所有CPU和GPU，都暗含了：CPU只有一种，GPU只有一种。实际上在这里可以换一种视角，将CPU和GPU看做GPU1和GPU2，这不过是异构资源的一种阐述方式。

1. 动机

    主要提及了
    1. 深度学习训练任务大多数都是资源可替换的，如：既可以用GPU，也可以用CPU训练。
    2. 不同任务在不同硬件上加速比不同。有的任务，如Bi-LSTM在CPU上甚至比GPU快。
    3. GPU虽然较快，但是贵且少。分析trace得出，大型集群中的CPU利用率较低。

2. 算法设计

    在任务随时可能到来时，在异构环境下（job有多种配置），最小化平均JCT已被证明是NP-hard问题。所以本文尝试首先将在一个简单情况下，尝试给出这种情况下的多项式时间最优解，随后再尝试将这个方法推广到全部情况。（算法设计思路）

    1. 在任务同时到达时，如何给出最小化JCT的最优解。

        首先强调假设条件：所有任务在一开始都到达了，并且每个任务只独占它的一份资源。

        算法主要分为三步：

        - 将问题转化为双向图匹配问题，生成该问题的输入参数
        - 解题
        - 将题解转化为可行的调度放置

        以下分别介绍：

        1. 将问题转化为双向图匹配问题，生成该问题的输入参数

            首先对JCT的计算方法进行了分析：

            每个计算资源对应的job队列中，从后往前数的第k个任务，它增加的总JCT为它的任务长度的k倍。（因为它后面的每个任务的JCT都要加上它的长度）

            接下来定义执行总时间矩阵：
            |  JOB  |  GPU  |  CPU  |
            | -- | -- | -- |
            |  1  |  3  |  4 |
            |  2  |  5  |  6 |

            以上矩阵给出了每个任务在不同资源上所花的时间。

            那么如上面例子，一共有两个任务，那么可以认为每个计算资源上的队列最长就为2。那么此时可以把每个队列（这里是CPU和GPU的队列）划分成为两个位置，将每个job安排到每个队列中的每个位置，且每个位置只能有一个job，那么这就可以成为一个合法的调度结果（空的位置填上一个假job即可）。

            而由我们前面对JCT计算的分析，实际上一个job如果确定了它放置在队列上的位置，就能计算出来它对整体JCT的增加值有多大。如：该job1放置在GPU队列的倒数第2位，则整体的JCT增加为2 * 3。

            那么这实际上就是一个最小化成本的双向图匹配问题：
            左侧的节点可以看做每个队列的位置，在上例中即为`CPU1, CPU2, GPU1, GPU2`，右侧为每个任务，这里就是`job1, job2`，那么每个job到每个位置都有一条边，这个边上的成本就是刚才我们分析得到的，该job增加的总JCT。

            这个双向图匹配问题可以用$O(n^3)$的算法得到解。

        2. 解题
    
            解题后，所有右侧节点能够找到唯一的左侧节点，并且其边上的权值最小。这些解出来的边，每个边就代表了一个job的放置位置。
    
        3. 将题解转化为可行的调度放置
    
            按照题解按顺序放置到对应机器上的队列即可。文中给出了证明，证明该题解在任务一开始就全部到达的条件下，能够得到最优解。
    
    2. 考虑任务online到达（转化到一般情况）
    
        本文不考虑抢占式调度，原因：
    
        1. k8s不支持
        2. 即使支持，迁移任务的overhead会很大。
    
        这里假定调度器对将来任务到达是不可知的。
    
        在online到达与先前的调度的区别是：
        Online到达时，某些资源会被任务占用。所以需要将当前正在运行的任务的剩余执行时间考虑进来。
    
        实际做法：在先前生成每个任务的增加JCT矩阵时，为它增加一个修正量，包含了对应队列上当前正在运行任务的剩余执行时间。剩余的解法没有任何变化。（EZ）
    
        当然，这时如果将全部任务直接静态调度到每个machine上是不合适的。这里的调度就变为了，每当有任务到来，或机器空闲时，使用该算法，找出那个优先级最高的，且对应机器空闲的任务，进行调度。
    
    3. 将多用户公平性考虑进来
    
        传统的资源分配问题中，在每个任务只有一种配置时，有以下主要的四个公平性性质：
    
        1. PE：用户不能以降低别的用户的性能为代价提高自己的性能。
        2. SI：每个用户的性能不能比独占1/n份的资源时更差。
        3. EF：没有用户在使用为其他用户分配的资源时会获得更好的吞吐率。
        4. SP：没有用户在谎称需要更多资源时会受益。 
    
        DRF论文中，在每个任务只有一种配置下，给出了满足这四个性质的公平性算法。但是本文证明了，在每个任务有多种配置时，（PE和PI）与（SP）不可能同时达到。
    
        随后给出了自己的方案：
    
        AlloX通过一些计算规则，为每个用户描述了一个任务执行的进度progress，用于描述每个用户的任务对集群的使用量。尽可能让大家的progress相等。
    
        - 方案：
    
            首先使用三元组（任务使用的计算资源数，任务主存使用量，任务耗时），用来表示某个任务在某个machine上的计算效率。
            
            分别在CPU和GPU上衡量。挑选消耗计算时间最短的（效率最高），计算得出它对资源的支配率 $di = max\{计算资源使用数/(集群总CPU|GPU数)，内存使用率/集群总内存\}$（并非显存，只考虑内存，因为独占GPU，不需要考虑它对显存的消耗，但是CPU内存是任务之间共享的。
    
            为每个job定义一个值，该值为前面计算的支配率di乘以一个折扣系数。
            
            如：任务i在GPU上比CPU效率更高，如果它现在就在GPU上运行，则该值就是di。否则，要将该值乘以$p_g/p_c$（在GPU上的执行时间除以在CPU上的执行时间）缩放到在CPU上。
            
            将这个值求和，就得到当前用户的progress。这个progress可以看做该用户的瞬时吞吐率。
    
        - 如何与调度算法结合：
    
            控制一个系数$\alpha \in [0, 1]$，该值乘以用户数，则得到一个缩放的用户数量。每次在调度时，只挑选progress最低的这些用户的任务。别的用户的任务根本不作为输入参数。
    
            随着用户的任务不断执行，每个用户的progress在变化。则progress较小的用户执行一部分任务后，它的progress会增大，则下一次调度时他拥有更小的机会被调度到。这样达成公平性。

    4. 实现
    
        基于k8s
    
        任务的长度profiler，依旧是按照job的训练epoch已知的情况下做的。


## Data sets & Experimental Design

- 实验设置：

    小规模集群：8个CPU worker，4个GPU worker，每个CPU和GPU都一样，由于不考虑分布式训练，所以不考虑网络带宽。

    模拟器：基于java。模拟器包含CPU和GPU，还涵盖了内存的设置。（本调度算法考虑到了内存，所以才需模拟）

- 比较方面

    - 模拟器的准确性

        分别在模拟器和实际集群跑同样的算法，最终效果差不多（是不是先用集群测量的时间作为模拟器的输入就行了？这个好像很容易啊）

    - 分别在集群和模拟器上比较

        包括JCT，利用率（CPU和GPU）

        并在时间序列上细粒度地给出每时刻的任务到达率，并分析每时刻的JCT，用户progress。

        是否存在饥饿现象。

    - 性能和公平性的trade-off

        发现将公平性参数$\alpha$设置为1时（完全不考虑公平性），则能够得到更好的性能，但是该参数越小，用户progress的方差越小。所以这存在trade-off。

    - profiler
      - 准确度
      - profile的overhead

## Conclusion And Future Work

<!-- 作者或者阅读者对本文工作的总结，以及未来可能的改进方向 -->
本文是首个在异构集群下，考虑任务有多种配置条件下的同时针对JCT和fairness的调度器。通过将问题转化成双向图匹配问题，有效地在某些限制条件下得到最优解。

但是本文仅针对单一任务独占单一运算器的情况，不支持分布式训练，以及space-sharing的情况，限制性较大。并且本文给出的公平性实现方案比较依赖上述条件，在复杂的分布式训练中很难扩展。

本文的调度算法，在一开始仅考虑任务全部同时来到的情况下，能够证明是最优解，但是将它拓展到online任务随时到达的情况时，没有给出数学分析，这实际上是一个贪婪的解。在这里存在的问题即为：后来的任务可能导致先前的调度决定变为一个次优的解。（这点在NSDI-2021-CoDDL中提出过）而本文又并不使用开销较大的抢占式调度（如何解决overhead是难题），是具有一些局限性的。