# Title

<!-- 此部分是论文标题-->
INFaaS: Automated Model-less Inference Serving

## Website
<!-- 网址，有DOI的建议用DOI地址-->
https://www.usenix.org/conference/atc21/presentation/romero

## Citing

<!-- 此部分是论文标题及其引用格式，建议使用latex格式 -->
@inproceedings{romero2021infaas,
  title={INFaaS: Automated Model-less Inference Serving},
  author={Romero, Francisco and Li, Qian and Yadwadkar, Neeraja J and Kozyrakis, Christos},
  booktitle={Proc. USENIX ATC},
  year={2021}
}

## Brief introduction

<!-- 通过三五句话描述这篇文章，包括 1. 论文的应用场景；2. 论文克服已有方法的局限性；3. 论文主要的技术手段； 4. 论文的预期结果 -->
分布式的深度学习模型推理服务系统为用户提供各式的推理服务。对于一类推理任务（如图像分类任务），用户可以选择的模型变体有上千种类型，它们的精度，时延，计算成本，内存消耗各有不同。已有的系统通常需要用户去指定一种固定的模型，不够灵活。INFaas提供了model-less的分布式推理服务，用户只需要提供他需要的模型精度、时延等模型指标，系统将自动找到合适的模型提供服务。面对生产环境的负载不确定性，它创新地提供垂直扩缩容，能够在扩缩容时更换为更合适的模型变体，从而提供细粒度的硬件资源管理，有效减少成本。为了提高资源利用率，系统支持多个推理任务共享一个计算资源，自动扩缩容也能够有效应对共享资源出现的推理性能下降问题。
系统预期能够提供更高的吞吐率，更低的硬件资源成本，更少的违反用户指定的时延、精度要求。

## Key Methology

<!-- 分点写，论述论文中主要技术手段的实施过程 -->
1. model-less的用户接口
   
用户在注册一种模型到系统中时，需要指定它的appID，appID相同的模型的服务类型完全相同，是可以被互相替代的。appID相同的不同模型即可认为是模型变体。
注册时需要提供一组验证集。系统收到模型后，对它用各种vendor-specfic的优化方案生成一批模型变体。在对应的机器上，使用验证集进行测试，计算出这个模型的profile信息。该信息包含了该模型变体的精度，时延，最高内存消耗，最高负载等信息，作为该模型变体在正常运行时的性能标准。

2. 整体架构
   
Controller在顶部负责接入、分发用户请求，并提供VM级别的扩缩容管理。
Worker为实际运行inference任务的一个VM，内部包含model级别的扩缩容管理；监控器记录Worker机器的资源状况。

3. 选择以及扩缩容模型变体的方案
   
    使用状态机描述Worker内的一个模型变体的状态，其内部包括
      - Inactive 表示该模型变体未加载到当前Worker
      - Active 表示已经加载到当前Worker
      - Interfered 表示该模型变体由于与它共享硬件资源的其他模型产生竞争，导致性能下降到非正常水平。
      - Overloaded 表示当前的请求负载已经超过它能承受的水平，导致了性能下降。
    1. 当用户请求到来时
   
      通过监控每个Worker上的模型变体的状态，Controller将用户请求分发到合适的Worker上（满足用户的latency和accuary要求）。
      当请求过程中出现Interfered或Overloaded等状态，系统将通过迁移或扩缩容的方法进行缓解。

    2. 当请求负载发生变化时，Worker管理的model级别扩缩容
      
        每个Worker内部会进行model级别的垂直-水平扩缩容。
        设计了ILP算法（核心内容）用于最小化扩缩容的成本，并且恰好满足用户需求。算法的输入是当前全部的机器状态，模型变体性能状态，输出为扩缩容指标。
        该算法给定了基于硬件资源的代价函数，并指定四个条件限制扩缩容指标范围。
        但是该算法是NP完全问题，所以采取了启发式的贪心算法以便在1秒内得到近似的结果。
        系统在挑选扩缩容指标时，考虑了不同的模型变体，从而细粒度地控制成本。（垂直扩缩容的含义）

    3. 当请求负载发生变化时，Controller管理的VM级别扩缩容
        
        Controller当检测一定比例的Worker都出现Overloaded或Interfered状态时，会唤醒一台新的VM加入Worker集群。
        方法与现有系统的类似。


## Data sets & Experimental Design

<!-- 撰写实验环境的设置，实验的对象，实验的比较方面，以及实验的结果（不要列举数据，要概括谈） -->
数据集：分别使用人造数据以及Twitter trace的真实数据。
1. 针对真实生产数据的整体评测。
    
    使用推特的真实数据，使用大量多种不同的模型，从整体上对比INFaas和现有产品的吞吐率，SLO率（违反用户要求概率）。
2. 针对模型变体选择以及扩缩容的测试
    
    只观察该指标，用单一的模型更好观察。
    使用人造的请求数据，分别为平稳的低负载，平稳的高负载，波动强烈的负载。观察扩缩容行为的异同。
    由于现有系统仅能支持一种底层硬件的扩缩容，所以分别针对CPU和GPU设置最优基线。
    而INFaas支持多种硬件的模型变体垂直扩缩容，能够自动根据负载调整到针对CPU或GPU的模型变体。
    结果：在负载低时能够自动降级缩容为CPU版本的模型变体，减少成本。在负载高时全程使用GPU版本的模型变体，保障性能。
    在负载波动时，能够随负载随时调整模型变体为最合适的，成本最低的模型变体，成本低。
3. 共享资源的效率测试
    
    针对共享GPU的测试。采用Inception-ResetNetV2和MobileNetV1两个在各方面都截然不同的模型共享GPU资源。
    使用Twitter的真实数据测试。
    共两块GPU，INFaas一开始当负载低时，只用一块GPU。当负载增高时，Worker进入Interfered状态，系统自动扩容到第二块GPU，扩容了模型的实例，分摊负载。


## Conclusion And Future Work

<!-- 作者或者阅读者对本文工作的总结，以及未来可能的改进方向 -->
INFaas将选择模型的职责揽入系统内部，用户只需关心推理任务的性能指标，无需关心具体使用哪个模型。其内部高效的模型变体选择与水平-垂直结合的扩缩容方案能够有效减少成本，提高吞吐率，减少SLO违反。
