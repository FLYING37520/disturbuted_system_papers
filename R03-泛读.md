# 算子自身资源调整

//机制：使用爬山算法预测算子的完成时间
@inproceedings{liu2019runtime,
  title={Runtime concurrency control and operation scheduling for high performance neural network training},
  author={Liu, Jiawen and Li, Dong and Kestor, Gokcen and Vetter, Jeffrey},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={188--199},
  year={2019},
  organization={IEEE}
}

//need to review.
@inproceedings{romero2021infaas,
  title={INFaaS: Automated Model-less Inference Serving},
  author={Romero, Francisco and Li, Qian and Yadwadkar, Neeraja J and Kozyrakis, Christos},
  booktitle={2021 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 21)},
  pages={397--411},
  year={2021}
}

//策略：基于DL模型训练过程中的资源交叠使用特点，细粒度分析DL算子运行时的交叠时机，预测最优的交叠位置，避免内存资源的过早分配，进一步提升GPU显存的使用效率，提供内存感知的安全资源分配。
@inproceedings{lim2021zico,
  title={Zico: Efficient $\{$GPU$\}$ Memory Sharing for Concurrent $\{$DNN$\}$ Training},
  author={Lim, Gangmuk and Ahn, Jeongseob and Xiao, Wencong and Kwon, Youngjin and Jeon, Myeongjae},
  booktitle={2021 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 21)},
  pages={161--175},
  year={2021}
}

//策略：使用白盒数据增量机制加速DL模型的训练过程。
@inproceedings{lee2021refurbish,
  title={Refurbish Your Training Data: Reusing Partially Augmented Samples for Faster Deep Neural Network Training},
  author={Lee, Gyewon and Lee, Irene and Ha, Hyeonmin and Lee, Kyunggeun and Hyun, Hwarim and Shin, Ahnjae and Chun, Byung-Gon},
  booktitle={2021 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 21)},
  pages={537--550},
  year={2021}
}

//机制+策略：根据DL训练时的周期迭代特性预测周期训练的时间；根据算子的伸缩特性预测DL模型在6个不同GPU型号上的运行时间。
@inproceedings{geoffrey2021habitat,
  title={Habitat: A Runtime-Based Computational Performance Predictor for Deep Neural Network Training},
  author={Geoffrey, X Yu and Gao, Yubo and Golikov, Pavel and Pekhimenko, Gennady},
  booktitle={2021 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 21)},
  pages={503--521},
  year={2021}
}

//机制：Mars使用图神经网络加速算子的设备放置过程，同时保障放置后的推理延迟。其首次将预训练的图编码器应用于强化学习框架，用于刻画算子的依赖结构；使用算子计算子图层次的序列放置器去学习最优的设备放置。对于达摩行能获得到接近27%的性能提升，对于小模型能快速得到最优放置结果。这是比较偏算法的工作，缺乏算子实际表现的启发式规则。
@article{lan2021accelerated,
  title={Accelerated Device Placement Optimization with Contrastive Learning},
  author={Lan, Hao and Chen, Li and Li, Baochun},
  year={2021} 
}

//Ray的分布式future机制为细粒度任务（function）的高效调度提供了支撑，能相交于单一RPC、分布式内存PRC、RPC+future三种方式提升更多的函数并行能力，同时同步开销。此外Ray的Ownership组件也提供了故障恢复等能力，未来可以进一步对TVM等算子进行分布式异构编译优化。
@inproceedings{wang2021ownership,
  title={Ownership: A Distributed Futures System for Fine-Grained Tasks.},
  author={Wang, Stephanie and Liang, Eric and Oakes, Edward and Hindman, Benjamin and Luan, Frank Sifei and Cheng, Audrey and Stoica, Ion},
  booktitle={NSDI},
  pages={671--686},
  year={2021}
}

//机制：Gillis使用无服务器计算模式来加速模型推理服务，由于无服务计算模型的受限资源使用和频繁的通信模式，存在服务器计算函数调度复杂、通信开销大、延迟和函数部署成本难以控制的问题。据此，Gillis通过fork join模式控制算子所在函数的调度顺序，使用动态规划方法评估不同算子划分组情况下的推理延迟，同时建模DL推理的运行成本，最后强化学习方法平衡成本和延迟。实验证明其具有一定的伸缩性，但其只在同构资源上验证了有效性。
@inproceedings{yu2021gillis,
  title={Gillis: Serving Large Neural Networks in Serverless Functions with Automatic Model Partitioning},
  author={Yu, Minchen and Jiang, Zhifeng and Ng, Hok Chun and Wang, Wei and Chen, Ruichuan and Li, Bo},
  booktitle={41st IEEE International Conference on Distributed Computing Systems},
  year={2021}
}

//机制：D^3首先根据模型局部算子的运行时间和层间算子的通信开销进行水平切分（划分为三个部分），然后针对边缘段的层内算子进行垂直切分，在满足推理延迟的同时，提升边缘端设备的资源使用效率，平均能缩短3.4倍的推理延迟和3.6倍的通信开销。
@article{zhang2021dynamic,
  title={Dynamic DNN Decomposition for Lossless Synergistic Inference},
  author={Zhang, Beibei and Xiang, Tian and Zhang, Hongxuan and Li, Te and Zhu, Shiqiang and Gu, Jianjun},
  journal={arXiv preprint arXiv:2101.05952},
  year={2021}
}

//机制：NPBench面向生物化学领域，定义了新的算子评估框架，其主要能够支持多种不同的编译平台，支持主流NP操作，通过注解的形式提供多重编译选项，其覆盖到了conv2D，lu等40多个主流算子。但不支持异构资源。
@inproceedings{ziogas2021npbench,
  title={NPBench: a benchmarking suite for high-performance NumPy},
  author={Ziogas, Alexandros Nikolaos and Ben-Nun, Tal and Schneider, Timo and Hoefler, Torsten},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={63--74},
  year={2021}
}

//策略：主要分析了NLP DL模型中算子的特点，包括算子依赖的多分支、计算和通信不能有效交叠这三方面， lbann使用数据与算子子图混合并行的方式提升transform模型的伸缩性。
@inproceedings{jain2021super,
  title={SUPER: SUb-Graph Parallelism for TransformERs},
  author={Jain, Arpan and Moon, Tim and Benson, Tom and Subramoni, Hari and Jacobs, Sam Ad{\'e} and Panda, Dhabaleswar K and Van Essen, Brian},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={629--638},
  year={2021},
  organization={IEEE}
}

//机制：Mopt从卷积算子计算逻辑的循环优化出发，指出张量不同循环层次的“贴片”使用不同内存（缓存）的情况是影响其自身执行时间的关键，其通过L1，L2，L3和内存之间数据移动的分析，建模数据传输的性能开销，简化最优贴片和变形最优配置的决策复杂度，能刻画传统黑盒方法受限搜索空间的局限性，相较于TVM能最多提升2倍的加速比。Mopt适用场景较为有限，目前只支持CPU和卷积算子，可以与TVM结合进一步提升其适用范围。
@inproceedings{li2021analytical,
  title={Analytical characterization and design space exploration for optimization of CNNs},
  author={Li, Rui and Xu, Yufan and Sukumaran-Rajam, Aravind and Rountev, Atanas and Sadayappan, P},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={928--942},
  year={2021}
}

# DL模型内不同算子的融合

//机制：现有算子融合方法仅是简单将不同算子运行逻辑端到端整合到一起，缺乏根据其自身运行逻辑的深度融合，本文将Relu作为mask过滤器与conv2D算子进行深度融合，预存权重与relu处理后结果，从而极大加速小模型的推理速度，同时层算子融合后也减少了算子数量。本方法适用于小模型同构资源的推理场景。
@inproceedings{olyaiy2021accelerating,
  title={Accelerating DNNs inference with predictive layer fusion},
  author={Olyaiy, MohammadHossein and Ng, Christopher and Lis, Mieszko},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={291--303},
  year={2021}
}



# 对关键路径上的算子进行切分融合，细粒度算子切分（阿里巴巴）

//策略：DL模型基于数据并行方式，长期运行时会面临通信开销大导致的伸缩性问题，本文提出使用分布式SGD算法提升算子状态同步的效率，使用算子稀疏矩阵的并行切分运行缩短完成时间，能够对典型CNN网络进行分布式加速。此文论证了细粒度的算子并行状态同步能相交于粗粒度的数据并行，能减少通信开销从而保障更好的CNN训练伸缩性。
@inproceedings{demirci2021partitioning,
  title={Partitioning sparse deep neural networks for scalable training and inference},
  author={Demirci, Gunduz Vehbi and Ferhatosmanoglu, Hakan},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={254--265},
  year={2021}
}

//机制：FastT将MS-DF简化为有向无环图结构，通过静态分析感知算子执行时的关键路径，并对关键路径上的算子依次进行调度，最后切分关键路径上的部分算子，以进一步缩短训练完成时间。
@inproceedings{yi2020fast,
  title={Fast Training of Deep Learning Models over Multiple GPUs},
  author={Yi, Xiaodong and Luo, Ziyue and Meng, Chen and Wang, Mengdi and Long, Guoping and Wu, Chuan and Yang, Jun and Lin, Wei},
  booktitle={Proceedings of the 21st International Middleware Conference},
  pages={105--118},
  year={2020}
}

//机制：MS-DF的任务需要其所处作业内的调度才能执行，任务中每个算子的调度则往往通过第三方的深度学习库来进行，这2级调度器之间实际资源使用的差异往往会导致资源浪费，Rammer即据此设计了算子集合任务和异构可用资源单元这两类抽象，在编译阶段进行算子内和算子间的调度优化，避免调度的开销；同时对GPU、TPU和IPU等资源进行统一接口的抽象。可通过算子内并行计划和算子间约束关系最小化运行时间。
@inproceedings{ma2020rammer,
  title={Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks},
  author={Ma, Lingxiao and Xie, Zhiqiang and Yang, Zhi and Xue, Jilong and Miao, Youshan and Cui, Wei and Hu, Wenxiang and Yang, Fan and Zhang, Lintao and Zhou, Lidong},
  booktitle={14th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 20)},
  pages={881--897},
  year={2020}
}


# 多个相同模型自身重复运行时相同算子融合

//机制：集群环境中存在多种使用单卡加速的DL模型，这些模型存在相同算子（处理逻辑，输入输出形状），进行跨模型的算子融合能够提升资源利用率和GPU、TPU的吞吐量；HFTA还同时提出对应的训练时超参数更新方法，从而保证收敛性。其适用于多个相似DL模型使用单卡资源反复执行时资源利用率提升的问题。
@article{wang2021horizontally,
  title={Horizontally Fused Training Array: An Effective Hardware Utilization Squeezer for Training Novel Deep Learning Models},
  author={Wang, Shang and Yang, Peiming and Zheng, Yuxuan and Li, Xin and Pekhimenko, Gennady},
  journal={arXiv preprint arXiv:2102.02344},
  year={2021}
}

//对不同的层次中的算子进行融合，充分评估的多个DL模型的内存占用开销，使用动态规划方法优化资源使用
@inproceedings{cai2021optimus,
  title={Optimus: towards optimal layer-fusion on deep learning processors},
  author={Cai, Xuyi and Wang, Ying and Zhang, Lei},
  booktitle={Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
  pages={67--79},
  year={2021}
}

//循环融合算子
@inproceedings{niu2021dnnfusion,
  title={DNNFusion: accelerating deep neural networks execution with advanced operator fusion},
  author={Niu, Wei and Guan, Jiexiong and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
  booktitle={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages={883--898},
  year={2021}
}

# 融合单个DL模型内部的相同算子

//模型：首次将延迟调度的思路引入算子粒度的调度，发现并行计算算子在不同执行时机对模型的数据流图有较大影响，并使用递归计算状态可变数据流子图最短完成时间达到整个大图的完成时间优化。该文献重点讨论了一个数据流内算子不同执行时机造成的影响，未来需要进一步考虑异构资源容量、通信开销、跨数据资源共享时对算子延迟调度造成的影响，或者一些新的共享效率提升潜力。此外，单一算子调度在50%的情况下并不能得到比TVM优化后更快的优化速度，未来可能需要两者结合，同时需要与rammer进行区分（overhead等方面考虑）。
@article{ding2021ios,
  title={Ios: Inter-operator scheduler for cnn acceleration},
  author={Ding, Yaoyao and Zhu, Ligeng and Jia, Zhihao and Pekhimenko, Gennady and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  year={2021}
}

# 多个相同模型自身错峰运行，并交叠资源

//策略：在cuda驱动层面上进行算子并行运行（空间复用）的研究，不关心算子层次优化的基础上，拦截算子对CUDA的调用，分析调用之间的依赖关系，从调用粒度的运行时解决单块GPU的资源复用问题，通过计算时间和通信时间的交叠比例，通信时间和计算时间的交叠比例来计算实际复用提升效果。从自定义CUDA驱动角度也能在一定程度上提升算子的执行效率，但仍然需要在多GPU环境，已经tensorflow、tvm等环境下适用性。
@inproceedings{parravicini2021dag,
  title={DAG-based Scheduling with Resource Sharing for Multi-task Applications in a Polyglot GPU Runtime},
  author={Parravicini, Alberto and Delamare, Arnaud and Arnaboldi, Marco and Santambrogio, Marco D},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={111--120},
  year={2021},
  organization={IEEE}
}

//模型：当前的GPU共享主要针对多DL模型，wavelet实现对同一模型在数据、模型并行时的资源共享，控制一个DL模型的训练执行时机来交替错峰使用GPU资源，同时实现内存（容量错峰）和计算资源（基于cuda stream实现计算错峰）的分时复用，提升资源使用率和运行完成时间。这种调度算法，假设算子划分已知和确定，不对算子进行调整优化，是tvm等工作的补充。
@article{wang2021wavelet,
  title={Wavelet: Efficient DNN Training with Tick-Tock Scheduling},
  author={Wang, Guanhua and Wang, Kehan and Jiang, Kenan and Li, Xiangjun and Stoica, Ion},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  year={2021}
}

# 不同模型共享内存资源，交叠资源

//机制：分析算子的内存资源使用特点，通过内存容量的复用，不同算子并行执行时可能会存在一定程度的交叠
@article{yu2019salus,
  title={Salus: Fine-grained gpu sharing primitives for deep learning applications},
  author={Yu, Peifeng and Chowdhury, Mosharaf},
  journal={arXiv preprint arXiv:1902.04610},
  year={2019}
}

# 分析单个模型的算子并行能力，进行异构资源交叠使用

@article{wang2020exploiting,
  title={Exploiting parallelism opportunities with deep learning frameworks},
  author={Wang, Yu Emma and Wu, Carole-Jean and Wang, Xiaodong and Hazelwood, Kim and Brooks, David},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={18},
  number={1},
  pages={1--23},
  year={2020},
  publisher={ACM New York, NY, USA}
}

# 通过算子计算图替换的方式来间接实现算子的融合替换

@inproceedings{jia2019taso,
  title={TASO: optimizing deep learning computation with automatic generation of graph substitutions},
  author={Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={47--62},
  year={2019}
}
