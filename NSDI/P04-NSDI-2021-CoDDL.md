# Title

<!-- 此部分是论文标题-->
Elastic Resource Sharing for Distributed Deep Learning

## Website
<!-- 网址，有DOI的建议用DOI地址-->
https://www.usenix.org/conference/nsdi21/presentation/hwang

## Citing

<!-- 引用格式，建议使用latex格式-->
@article{shin2021elastic,
  title={Elastic Resource Sharing for Distributed Deep Learning},
  author={Shin, Jinwoo and Park, KyoungSoo},
  year={2021}
}

## Brief Introduction

<!-- 通过三五句话描述这篇文章，包括 1. 论文的应用场景；2. 论文克服已有方法的局限性；3. 论文主要的技术手段； 4. 论文的预期结果 -->
本文针对大规模集群深度学习模型训练场景，提供高效的弹性资源调度。
已有调度算法通常更倾向于调度更早结束的任务，并且不能够在运行时调整训练任务资源。这些限制会导致更长的JCT。本文提出AFS--一个启发式的调度算法，它不仅关注任务时长，还关注任务的资源利用效率，并且针对未来任务到来的不可知性，做了启发式假设。设计并实现了CoDDL系统，实现该调度算法，并且支持自动模型并行训练，以及训练任务的资源热调整。最终该调度算法在JCT和资源利用率等方面都能取得进步。

## Key Methodology

<!-- 分点写，论述论文中主要技术手段的实施过程 -->
文章主体内容分三大模块。
1. 证明资源利用效率的重要性。
   
   对于随资源量提升，吞吐量线性提升的job来说，短作业优先已经能够提供最优JCT。然而DLT任务（深度学习模型训练）是亚线性的，也就是每个任务在获得额外资源时，能够获得的效率提升是有区别的。通过针对先前的调度算法，在低竞争和高竞争的场景下做测试，证明了结论。

2. 调度算法设计
   
   该调度算法的设计思路是努力去平衡短的和资源利用高效的任务的优先级。
   
   算法从简单情况入手。首先在只有两个任务的情况下，描述贪心算法的调度策略。在这时得到调度贪心最优解。但是当出现第三个任务时，该贪心解可能变成更差的解。分析得出：当存在竞争时，每次调度尽可能保持旧的资源配给不变更容易取得更优的解。并给出了任意两个任务的优先级比较公式。

   当扩展到n个任务时，算法证明了上面提出的优先级比较公式是可传递的。则在任意需要调度的时刻，ASF能够在一批任务中，找到优先级最高的任务。将1个GPU分配给它，分配完之后，重新对这一批任务计算优先级，再将下一个GPU分配给最高优先级任务。从而得到分配结果。

3. CoDDL框架设计与实现

    文章实现了CoDDL用于管理一个深度学习训练集群。它的工程难点为：用户无需编写分布式训练代码即可进行数据并行训练，并且支持深度学习训练任务的高效资源热调整。

    1. 自动并行训练

        用户只需编写针对单一GPU的训练代码，该系统能够自动将模型修改为支持数据并行的训练。
        
        具体方法为：在计算图中插入一些顶点，能够在梯度计算完成后，通知该GPU对应的通讯栈，该通讯栈与其他GPU worker的通讯栈通信，合并梯度，再更新参数。

    2. 高效的资源热调整

       由于调度算法的资源调整很频繁，所有这个特性很重要。

       如何实现：当一个job分配更多的GPU时，旧的GPU继续运算，新的GPU开始准备这个模型（加载图，从其他worker拉取最新的参数），当彻底准备好后，告知其他worker的通信栈有新的GPU加入。（这里估计有更细节的同步机制）

    3. 重新配置请求突增的处理

        当多个任务同时完成时，可能突然触发大量的调度请求。本文的策略是：每个资源调整的请求直接分发到worker上，新到的资源调整能够让旧的正在运行的资源调整任务取消。

    4. GPU配给策略，避免跨机器的训练

        使用network packing机制。每个任务只能分配得到每个机器GPU个数的因子或整数倍个GPU。如每个机器4个GPU，则任务能够得到 1/2/4n 个GPU。这样，每个机器上最多有一个任务能够跨机器训练，保证了网络带宽独占。

    5. 吞吐率测量

        AFS算法需要得知每个任务在不同GPU下的吞吐率。本文使用`overestimate-firstand-re-adjust-later`的方法测量。即首先给一个任务分配较多的GPU，再通过使用快速资源调整的方式，分别测量不同GPU个数时，该任务的吞吐量。


## Data sets & Experimental Design

<!-- 撰写实验环境的设置，实验的对象，实验的比较方面，以及实验的结果（不要列举数据，要概括谈） -->
数据集：

使用微软的137天的真实训练记录数据。

关注指标：
平均JCT，排队任务长度，排队等待时长，集群效率，GPU利用率。

1. 使用模拟器测试
2. 在真实数据上测试
    
    关注调度算法的性能，以及训练任务资源热更新的效率。

3. 针对最末位JCT的评估。
4. 针对利他主义方法的比较。

## Conclusion And Future Work

<!-- 作者或者阅读者对本文工作的总结，以及未来可能的改进方向 -->
1. 弹性资源调度算法在同构资源上能够有效减少平均JCT，同时考虑任务长度以及资源利用效率。启发式的算法，使得它能够将未来任务的成本分摊到当前的资源调度中。
2. 给出自动数据并行的模型训练实现，给出支持训练任务的资源配给热更新，这些在工程上比较困难。
3. 仅支持数据并行。
4. 仅关注JCT指标。
5. 训练资源分配在该算法中仅以GPU为单位，过于粗粒度。
6. 本文的算法只能在同构环境上使用。在异构环境上，每个训练任务在不同硬件上的吞吐率各有不同，资源利用效率不同，运行时长也不同，从而不能使用本文调度算法中的数学建模进行套用。
